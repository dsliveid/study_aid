# 实时语音识别功能技术文档

## 目录

1. [需求分析](#1-需求分析)
2. [实现流程](#2-实现流程)
3. [技术原理](#3-技术原理)
4. [技术选型](#4-技术选型)
5. [实现方案](#5-实现方案)
6. [开发指南](#6-开发指南)

---

## 1. 需求分析

### 1.1 功能需求

#### 核心功能
- **实时语音识别**（Real-time Speech Recognition）
- **双音频源支持**
  - 麦克风音频采集
  - 系统音频采集（电脑本身的声音）
- **平台支持**：Windows 10

#### 性能需求
| 指标 | 要求 |
|------|------|
| 实时性 | 低延迟，接近实时识别 |
| 准确性 | 高识别准确率（>90%） |
| 稳定性 | 长时间运行不崩溃 |
| 资源占用 | 合理的CPU和内存使用 |

#### 用户体验需求
- 音频源切换便捷
- 识别结果实时显示
- 支持暂停/开始控制
- 音量调节功能

### 1.2 技术约束

#### 平台约束
- 操作系统：Windows 10
- 音频API：Windows WASAPI（Windows Audio Session API）

#### 音频源特性
- **麦克风**：外部音频输入设备
- **系统音频**：Windows WASAPI Loopback模式捕获

### 1.3 识别需求

#### 语言支持
- 中文识别（普通话）
- 英文识别
- 中英混合识别

#### 识别模式
- 连续语音识别
- 实时流式处理
- 语音转文字输出

---

## 2. 实现流程

### 2.1 整体架构

```
用户界面层
    ↓
音频源选择层
    ↓
音频采集层
    ↓
音频预处理层
    ↓
语音识别层
    ↓
结果输出层
```

### 2.2 详细实现步骤

#### 阶段一：环境准备与依赖安装

**1. 开发环境搭建**
```bash
# 创建Python虚拟环境
python -m venv venv
venv\Scripts\activate

# 升级pip
pip install --upgrade pip
```

**2. 依赖库安装**
```bash
# 音频采集库
pip install pyaudio sounddevice

# 语音识别库
pip install faster-whisper vosk

# 音频处理库
pip install numpy scipy librosa

# UI框架
pip install PyQt5
```

#### 阶段二：音频采集模块开发

**1. 麦克风音频采集**
- 枚举可用音频设备
- 配置音频参数（采样率、声道、位深）
- 实现实时音频流读取

**2. 系统音频采集**
- 使用WASAPI捕获系统音频
- 配置音频会话
- 处理音频权限问题

#### 阶段三：音频预处理

**1. 音频格式转换**
- 统一音频格式（PCM, WAV）
- 采样率转换（如需要）
- 声道处理（单声道/立体声）

**2. 音频增强**
- 噪声抑制
- 音量归一化
- 静音检测（VAD）

#### 阶段四：语音识别引擎集成

**1. 识别引擎选择**
- 本地识别引擎（离线）
- 云端识别引擎（在线）
- 混合方案

**2. 实时识别实现**
- 流式音频输入
- 分块识别处理
- 结果实时返回

#### 阶段五：用户界面开发

**1. 音频源选择界面**
- 麦克风设备列表
- 系统音频选项
- 音频源切换

**2. 控制界面**
- 开始/停止按钮
- 音量控制
- 识别结果显示

#### 阶段六：测试与优化

**1. 功能测试**
- 麦克风识别测试
- 系统音频识别测试
- 音频源切换测试

**2. 性能优化**
- 降低延迟
- 优化资源占用
- 提升识别准确率

### 2.3 关键技术点

#### 音频源管理
- 设备枚举与选择
- 音频流管理
- 错误处理与恢复

#### 实时处理
- 多线程/异步处理
- 音频缓冲区管理
- 流式识别接口

#### 权限管理
- Windows音频权限
- 麦克风访问权限
- 系统音频捕获权限

---

## 3. 技术原理

### 3.1 音频采集原理

#### 3.1.1 音频信号基础

**数字音频表示**

| 参数 | 说明 | 常用值 |
|------|------|--------|
| 采样率 | 每秒采样次数 | 8000, 16000, 44100, 48000 Hz |
| 位深 | 每个采样点的精度 | 16-bit, 24-bit, 32-bit |
| 声道数 | 音频通道数 | 1（单声道）, 2（立体声） |

**音频数据格式**
```
PCM (Pulse Code Modulation) - 原始音频数据
├── 16-bit PCM - 2字节/采样
├── 24-bit PCM - 3字节/采样
└── 32-bit Float PCM - 4字节/采样
```

#### 3.1.2 Windows音频子系统

**WASAPI（Windows Audio Session API）**

| 模式 | 说明 | 用途 |
|------|------|------|
| 共享模式 | 多个应用共享音频设备 | 普通应用 |
| 独占模式 | 应用独占音频设备 | 专业音频应用 |
| 环回模式 | 捕获系统输出音频 | 系统音频录制 |

**音频设备层次结构**
```
MMDeviceEnumerator (设备枚举器)
    ↓
IMMDevice (音频设备)
    ↓
IAudioClient (音频客户端)
    ↓
IAudioCaptureClient (音频捕获客户端)
```

#### 3.1.3 麦克风音频采集原理

**采集流程**
1. 设备枚举：获取所有可用的音频输入设备
2. 设备选择：用户选择目标麦克风设备
3. 音频会话创建：创建IAudioClient接口
4. 流格式配置：设置采样率、位深、声道数
5. 缓冲区分配：分配音频缓冲区
6. 实时采集：循环读取音频数据
7. 数据处理：对采集的音频数据进行处理

**关键代码逻辑**
```python
while recording:
    获取可用缓冲区大小
    读取音频数据到缓冲区
    处理音频数据（预处理）
    发送到识别引擎
```

#### 3.1.4 系统音频采集原理

**WASAPI Loopback模式**
- 捕获系统正在播放的音频
- 不影响系统音频输出
- 需要管理员权限（某些情况）

**采集流程**
1. 枚举音频输出设备：扬声器、耳机等
2. 启用Loopback模式：配置为环回捕获
3. 创建音频会话：使用WASAPI接口
4. 实时捕获：捕获系统输出的音频流
5. 混合处理：如果需要，可以混合多个音频源

**技术难点**
- 需要处理DRM保护音频（可能无法捕获）
- 不同应用的音频隔离
- 音频延迟问题

### 3.2 语音识别原理

#### 3.2.1 语音识别流程

**传统识别流程**
```
音频输入
    ↓
预处理（降噪、VAD）
    ↓
特征提取（MFCC）
    ↓
声学模型（AM）
    ↓
语言模型（LM）
    ↓
解码器
    ↓
文本输出
```

**深度学习识别流程**
```
音频输入
    ↓
预处理
    ↓
特征提取
    ↓
神经网络模型（End-to-End）
    ↓
序列解码
    ↓
文本输出
```

#### 3.2.2 音频预处理

**1. 预加重（Pre-emphasis）**
- 提升高频分量
- 公式：y[n] = x[n] - αx[n-1]，α ≈ 0.97

**2. 分帧（Framing）**
- 将连续音频分成短时帧
- 帧长：通常20-30ms
- 帧移：通常10ms（重叠50%）

**3. 加窗（Windowing）**
- 减少帧边界的不连续性
- 常用窗函数：汉明窗（Hamming Window）

**4. 语音活动检测（VAD）**
- 检测是否有语音活动
- 过滤静音段
- 减少计算量

#### 3.2.3 特征提取

**MFCC（Mel-Frequency Cepstral Coefficients）**
```
音频帧 → FFT → Mel滤波器组 → DCT → MFCC系数
```

**步骤**
1. 快速傅里叶变换（FFT）：时域→频域
2. Mel滤波器组：模拟人耳听觉特性
3. 对数能量：取对数压缩动态范围
4. 离散余弦变换（DCT）：去相关，降维

**常用特征**
- MFCC：13维静态特征 + 13维一阶差分 + 13维二阶差分
- Fbank（Filter bank）：40-80维

#### 3.2.4 声学模型

**传统模型**
- **GMM-HMM**：高斯混合模型 + 隐马尔可夫模型
- 优点：成熟稳定
- 缺点：准确率较低，需要大量训练数据

**深度学习模型**
- **DNN-HMM**：深度神经网络替代GMM
- **CNN**：卷积神经网络，提取局部特征
- **RNN/LSTM**：循环神经网络，处理时序依赖
- **Transformer**：自注意力机制，长距离依赖
- **Conformer**：CNN + Transformer混合

**End-to-End模型**
- **CTC（Connectionist Temporal Classification）**：无需对齐
- **LAS（Listen, Attend and Spell）**：编码器-解码器架构
- **RNN-Transducer**：流式识别
- **Whisper**：OpenAI的大规模预训练模型

#### 3.2.5 语言模型

**N-gram语言模型**
- 基于统计的语言模型
- 考虑词的上下文关系
- 公式：P(wi|wi-n+1, ..., wi-1)

**神经网络语言模型**
- **RNN-LM**：循环神经网络语言模型
- **Transformer-LM**：基于Transformer的语言模型
- **BERT**：双向编码器表示

#### 3.2.6 解码器

**解码算法**
- **Viterbi算法**：寻找最优路径
- **Beam Search**：束搜索，保留多个候选
- **集束搜索优化**：长度归一化、覆盖惩罚

**实时解码**
- **流式识别**：边采集边识别
- **延迟优化**：减少识别延迟
- **增量解码**：逐步更新识别结果

### 3.3 实时处理原理

#### 3.3.1 多线程架构

**生产者-消费者模式**
```
音频采集线程（生产者）
    ↓ 音频缓冲队列
音频处理线程（消费者）
    ↓ 特征队列
识别线程（消费者）
    ↓ 结果队列
UI更新线程（消费者）
```

#### 3.3.2 缓冲区管理

**环形缓冲区（Ring Buffer）**
- 固定大小的循环缓冲区
- 避免频繁内存分配
- 高效的数据读写

**缓冲策略**
- **双缓冲**：两个缓冲区交替使用
- **三缓冲**：三个缓冲区，减少等待
- **动态缓冲**：根据负载调整缓冲区大小

#### 3.3.3 同步机制

**线程同步**
- **互斥锁（Mutex）**：保护共享资源
- **条件变量（Condition Variable）**：线程间通知
- **信号量（Semaphore）**：控制并发数量

**无锁编程**
- **原子操作**：CAS（Compare-And-Swap）
- **无锁队列**：避免锁开销

---

## 4. 技术选型

### 4.1 开发语言选择

#### 4.1.1 Python（推荐）

**优势**
- 丰富的音频处理库
- 成熟的语音识别库
- 快速开发，易于维护
- 跨平台支持

**适用场景**
- 快速原型开发
- 中小型项目
- 需要快速迭代

**主要库**
- 音频采集：pyaudio, sounddevice, pyaudio-portaudio
- 语音识别：SpeechRecognition, vosk, whisper, faster-whisper
- 音频处理：librosa, numpy, scipy

#### 4.1.2 C++

**优势**
- 高性能，低延迟
- 直接访问Windows API
- 资源占用少
- 适合嵌入式

**劣势**
- 开发周期长
- 库支持相对较少
- 学习曲线陡峭

**适用场景**
- 高性能要求
- 商业产品
- 需要极致优化

#### 4.1.3 C# (.NET)

**优势**
- Windows原生支持
- NAudio库功能强大
- 良好的UI框架（WPF/WinForms）
- 企业级应用

**适用场景**
- Windows桌面应用
- 企业级项目
- 需要精美UI

### 4.2 音频采集技术选型

#### 4.2.1 PyAudio

**特点**
- PortAudio的Python绑定
- 跨平台支持
- 功能全面

**优势**
- 成熟稳定
- 社区活跃
- 文档完善

**劣势**
- Windows安装可能需要编译
- 对WASAPI支持有限

**适用**：麦克风音频采集

**示例代码**
```python
import pyaudio

p = pyaudio.PyAudio()
stream = p.open(format=pyaudio.paInt16,
                channels=1,
                rate=16000,
                input=True,
                frames_per_buffer=1024)
```

#### 4.2.2 SoundDevice

**特点**
- PortAudio的现代Python接口
- 更简洁的API
- 支持异步操作

**优势**
- 易于使用
- 支持回调模式
- 良好的NumPy集成

**适用**：麦克风音频采集

#### 4.2.3 PyAudio WASAPI

**特点**
- 支持WASAPI
- 可捕获系统音频
- Windows专用

**优势**
- 原生Windows支持
- 低延迟
- 支持Loopback模式

**适用**：系统音频采集

#### 4.2.4 NAudio（C#）

**特点**
- .NET音频库
- 功能强大
- WASAPI完整支持

**优势**
- Windows原生
- API友好
- 支持所有WASAPI模式

**适用**：C#项目，系统音频采集

### 4.3 语音识别引擎选型

#### 4.3.1 Vosk（推荐离线方案）

**特点**
- 离线语音识别
- 支持多语言
- 轻量级
- 流式识别

**优势**
- 完全离线，无需网络
- 低延迟
- 支持实时识别
- 模型大小可选（小到几十MB）

**劣势**
- 准确率略低于云端方案
- 需要下载语言模型

**适用场景**
- 隐私敏感应用
- 网络不稳定环境
- 需要离线运行

**技术规格**
| 参数 | 值 |
|------|-----|
| 支持语言 | 中文、英文等20+语言 |
| 模型大小 | 50MB - 1.5GB |
| 识别速度 | 实时 |
| 准确率 | 90-95% |

**示例代码**
```python
from vosk import Model, KaldiRecognizer
import json

model = Model("model-cn")
rec = KaldiRecognizer(model, 16000)

while True:
    data = stream.read(4096)
    if rec.AcceptWaveform(data):
        result = json.loads(rec.Result())
        print(result["text"])
```

#### 4.3.2 Whisper（OpenAI）

**特点**
- 大规模预训练模型
- 多语言支持
- 高准确率
- 支持实时识别

**优势**
- 识别准确率极高
- 支持中英混合
- 强大的泛化能力
- 开源免费

**劣势**
- 模型较大（最小74MB，最大6GB）
- 计算资源要求高
- 延迟相对较高

**适用场景**
- 高准确率要求
- 有充足计算资源
- 不介意延迟

**模型规格**
| 模型 | 大小 | 速度 | 准确率 |
|------|------|------|--------|
| tiny | 74MB | 最快 | 中等 |
| base | 142MB | 快 | 较高 |
| small | 461MB | 中等 | 高 |
| medium | 1.5GB | 慢 | 很高 |
| large | 2.9GB | 最慢 | 最高 |

**优化方案**
- faster-whisper：CTranslate2优化，速度提升5倍
- whisper.cpp：C++实现，支持量化

**示例代码**
```python
import whisper

model = whisper.load_model("base")
result = model.transcribe("audio.wav", language="zh")
print(result["text"])
```

#### 4.3.3 Faster-Whisper（推荐）

**特点**
- Whisper的优化版本
- CTranslate2后端
- 更快、更省内存

**优势**
- 速度提升4-5倍
- 内存占用减少
- 支持GPU加速
- 兼容Whisper模型

**适用场景**
- 需要Whisper的高准确率
- 对性能有要求
- 实时识别

**示例代码**
```python
from faster_whisper import WhisperModel

model = WhisperModel("base", device="cpu", compute_type="int8")
segments, info = model.transcribe("audio.wav", language="zh")

for segment in segments:
    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
```

#### 4.3.4 SpeechRecognition（云端方案）

**特点**
- 统一接口
- 支持多个云端引擎
- 易于使用

**支持的引擎**
- Google Web Speech API（免费，有限制）
- Google Cloud Speech API（付费）
- Azure Speech API（付费）
- IBM Watson Speech API（付费）
- Wit.ai（Facebook，免费）

**优势**
- 使用简单
- 准确率高
- 无需本地模型

**劣势**
- 需要网络连接
- 延迟较高
- 隐私问题
- 可能产生费用

**适用场景**
- 网络稳定
- 不介意隐私
- 快速原型

**示例代码**
```python
import speech_recognition as sr

r = sr.Recognizer()
with sr.Microphone() as source:
    audio = r.listen(source)
    text = r.recognize_google(audio, language="zh-CN")
    print(text)
```

#### 4.3.5 百度语音识别（国内方案）

**特点**
- 中文优化
- 支持实时识别
- 有免费额度

**优势**
- 中文识别准确率高
- 国内访问快
- 支持方言
- 文档完善

**劣势**
- 需要网络
- 有调用限制
- 需要注册账号

**适用场景**
- 中文为主
- 国内使用
- 有网络连接

### 4.4 技术栈推荐方案

#### 方案一：Python + Vosk（离线方案）

**技术栈**
- 开发语言：Python 3.8+
- 音频采集：pyaudio / sounddevice
- 系统音频：pyaudio + WASAPI扩展
- 语音识别：vosk
- UI框架：PyQt5 / Tkinter
- 音频处理：numpy, scipy

**优势**
- 完全离线
- 低延迟
- 资源占用小
- 开发快速

**劣势**
- 准确率中等
- 需要下载模型

**适用场景**
- 隐私敏感
- 离线环境
- 快速开发

#### 方案二：Python + Faster-Whisper（高准确率方案）

**技术栈**
- 开发语言：Python 3.8+
- 音频采集：pyaudio / sounddevice
- 系统音频：pyaudio + WASAPI
- 语音识别：faster-whisper
- UI框架：PyQt5
- 加速：CUDA（可选）

**优势**
- 高准确率
- 支持中英混合
- 速度优化
- 开源免费

**劣势**
- 模型较大
- 计算资源要求高

**适用场景**
- 高准确率要求
- 有充足计算资源
- 中英混合识别

#### 方案三：C# + NAudio + Azure（企业方案）

**技术栈**
- 开发语言：C# (.NET 6+)
- 音频采集：NAudio
- 语音识别：Azure Speech SDK
- UI框架：WPF / WinUI 3
- 架构：MVVM

**优势**
- Windows原生
- 企业级支持
- 高准确率
- 功能完善

**劣势**
- 需要付费
- 需要网络
- 开发周期长

**适用场景**
- 企业应用
- 商业产品
- Windows平台

### 4.5 技术选型对比

| 方案 | 准确率 | 延迟 | 离线 | 成本 | 开发难度 |
|------|--------|------|------|------|----------|
| Python + Vosk | 中 | 低 | 是 | 免费 | 低 |
| Python + Faster-Whisper | 高 | 中 | 是 | 免费 | 中 |
| Python + SpeechRecognition | 高 | 高 | 否 | 免费/付费 | 低 |
| C# + Azure | 高 | 中 | 否 | 付费 | 中 |

---

## 5. 实现方案

### 5.1 推荐方案：Python + Faster-Whisper

#### 5.1.1 项目结构

```
study_aid3/
├── speech_recognition/
│   ├── __init__.py
│   ├── audio_capture.py          # 音频采集模块
│   ├── audio_processor.py        # 音频处理模块
│   ├── speech_recognizer.py      # 语音识别模块
│   ├── ui/
│   │   ├── __init__.py
│   │   ├── main_window.py        # 主窗口
│   │   └── widgets.py            # 自定义控件
│   ├── models/                   # 模型目录
│   └── utils/
│       ├── __init__.py
│       └── audio_utils.py        # 音频工具函数
├── requirements.txt
└── README.md
```

#### 5.1.2 核心模块实现

**音频采集模块（audio_capture.py）**

```python
import pyaudio
import sounddevice as sd
import numpy as np
from typing import Optional, Callable
import queue


class AudioCapture:
    def __init__(self, sample_rate: int = 16000, channels: int = 1):
        self.sample_rate = sample_rate
        self.channels = channels
        self.audio_queue = queue.Queue(maxsize=100)
        self.is_recording = False
        self.stream = None

    def list_microphone_devices(self) -> list:
        """列出所有麦克风设备"""
        devices = sd.query_devices()
        mic_devices = []
        for i, device in enumerate(devices):
            if device['max_input_channels'] > 0:
                mic_devices.append({
                    'index': i,
                    'name': device['name'],
                    'channels': device['max_input_channels']
                })
        return mic_devices

    def list_system_audio_devices(self) -> list:
        """列出所有系统音频输出设备"""
        devices = sd.query_devices()
        sys_devices = []
        for i, device in enumerate(devices):
            if device['max_output_channels'] > 0:
                sys_devices.append({
                    'index': i,
                    'name': device['name'],
                    'channels': device['max_output_channels']
                })
        return sys_devices

    def start_microphone_capture(self, device_index: Optional[int] = None):
        """开始麦克风采集"""
        def callback(indata, frames, time, status):
            if status:
                print(f"Audio callback status: {status}")
            self.audio_queue.put(indata.copy())

        self.stream = sd.InputStream(
            device=device_index,
            channels=self.channels,
            samplerate=self.sample_rate,
            callback=callback
        )
        self.stream.start()
        self.is_recording = True

    def start_system_audio_capture(self, device_index: int):
        """开始系统音频采集（WASAPI Loopback）"""
        def callback(indata, frames, time, status):
            if status:
                print(f"Audio callback status: {status}")
            self.audio_queue.put(indata.copy())

        self.stream = sd.InputStream(
            device=device_index,
            channels=2,  # 系统音频通常是立体声
            samplerate=44100,
            callback=callback
        )
        self.stream.start()
        self.is_recording = True

    def stop_capture(self):
        """停止采集"""
        if self.stream:
            self.stream.stop()
            self.stream.close()
            self.stream = None
        self.is_recording = False

    def get_audio_data(self, timeout: float = 1.0) -> Optional[np.ndarray]:
        """获取音频数据"""
        try:
            return self.audio_queue.get(timeout=timeout)
        except queue.Empty:
            return None
```

**语音识别模块（speech_recognizer.py）**

```python
from faster_whisper import WhisperModel
import numpy as np
from typing import Iterator, Optional
import threading
import queue


class SpeechRecognizer:
    def __init__(self, model_size: str = "base", device: str = "cpu"):
        self.model = WhisperModel(
            model_size,
            device=device,
            compute_type="int8"
        )
        self.is_running = False
        self.result_queue = queue.Queue()
        self.recognize_thread = None

    def recognize_stream(self, audio_stream: Iterator[np.ndarray], language: str = "zh"):
        """流式识别"""
        audio_buffer = []
        buffer_duration = 0

        for audio_chunk in audio_stream:
            if not self.is_running:
                break

            audio_buffer.append(audio_chunk)
            chunk_duration = len(audio_chunk) / 16000
            buffer_duration += chunk_duration

            # 每3秒识别一次
            if buffer_duration >= 3.0:
                audio_data = np.concatenate(audio_buffer)

                segments, info = self.model.transcribe(
                    audio_data,
                    language=language,
                    beam_size=5,
                    vad_filter=True,
                    vad_parameters=dict(min_silence_duration_ms=500)
                )

                for segment in segments:
                    self.result_queue.put({
                        'text': segment.text,
                        'start': segment.start,
                        'end': segment.end
                    })

                audio_buffer = []
                buffer_duration = 0

    def start_recognize(self, audio_stream: Iterator[np.ndarray], language: str = "zh"):
        """启动识别线程"""
        self.is_running = True
        self.recognize_thread = threading.Thread(
            target=self.recognize_stream,
            args=(audio_stream, language)
        )
        self.recognize_thread.start()

    def stop_recognize(self):
        """停止识别"""
        self.is_running = False
        if self.recognize_thread:
            self.recognize_thread.join()
            self.recognize_thread = None

    def get_result(self, timeout: float = 1.0) -> Optional[dict]:
        """获取识别结果"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None
```

**音频处理模块（audio_processor.py）**

```python
import numpy as np
from scipy import signal
from typing import Optional


class AudioProcessor:
    @staticmethod
    def resample(audio: np.ndarray, original_rate: int, target_rate: int) -> np.ndarray:
        """重采样"""
        if original_rate == target_rate:
            return audio
        number_of_samples = round(len(audio) * float(target_rate) / original_rate)
        resampled_audio = signal.resample(audio, number_of_samples)
        return resampled_audio.astype(np.float32)

    @staticmethod
    def convert_to_mono(audio: np.ndarray) -> np.ndarray:
        """转换为单声道"""
        if len(audio.shape) == 1:
            return audio
        return np.mean(audio, axis=1)

    @staticmethod
    def normalize(audio: np.ndarray) -> np.ndarray:
        """音量归一化"""
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            return audio / max_val
        return audio

    @staticmethod
    def apply_highpass_filter(audio: np.ndarray, sample_rate: int, cutoff: int = 80) -> np.ndarray:
        """高通滤波器，去除低频噪声"""
        nyquist = 0.5 * sample_rate
        normal_cutoff = cutoff / nyquist
        b, a = signal.butter(5, normal_cutoff, btype='high', analog=False)
        filtered_audio = signal.filtfilt(b, a, audio)
        return filtered_audio

    @staticmethod
    def process_audio(audio: np.ndarray, original_rate: int, target_rate: int = 16000) -> np.ndarray:
        """完整的音频处理流程"""
        # 转换为单声道
        audio = AudioProcessor.convert_to_mono(audio)

        # 重采样
        audio = AudioProcessor.resample(audio, original_rate, target_rate)

        # 归一化
        audio = AudioProcessor.normalize(audio)

        # 高通滤波
        audio = AudioProcessor.apply_highpass_filter(audio, target_rate)

        return audio
```

**主窗口（main_window.py）**

```python
import sys
from PyQt5.QtWidgets import (QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,
                             QPushButton, QComboBox, QTextEdit, QLabel, QSlider)
from PyQt5.QtCore import QThread, pyqtSignal
from ..audio_capture import AudioCapture
from ..speech_recognizer import SpeechRecognizer
from ..audio_processor import AudioProcessor


class AudioWorker(QThread):
    text_received = pyqtSignal(str)

    def __init__(self, audio_capture, recognizer):
        super().__init__()
        self.audio_capture = audio_capture
        self.recognizer = recognizer
        self.running = False

    def run(self):
        self.running = True
        while self.running:
            audio_data = self.audio_capture.get_audio_data()
            if audio_data is not None:
                processed_audio = AudioProcessor.process_audio(
                    audio_data,
                    original_rate=self.audio_capture.sample_rate
                )
                result = self.recognizer.get_result()
                if result:
                    self.text_received.emit(result['text'])

    def stop(self):
        self.running = False


class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("实时语音识别")
        self.setGeometry(100, 100, 800, 600)

        self.audio_capture = AudioCapture()
        self.recognizer = SpeechRecognizer()
        self.audio_worker = None

        self.init_ui()
        self.load_devices()

    def init_ui(self):
        central_widget = QWidget()
        self.setCentralWidget(central_widget)

        layout = QVBoxLayout()

        # 音频源选择
        source_layout = QHBoxLayout()
        source_label = QLabel("音频源:")
        self.source_combo = QComboBox()
        self.source_combo.addItem("麦克风")
        self.source_combo.addItem("系统音频")
        source_layout.addWidget(source_label)
        source_layout.addWidget(self.source_combo)
        layout.addLayout(source_layout)

        # 设备选择
        device_layout = QHBoxLayout()
        device_label = QLabel("设备:")
        self.device_combo = QComboBox()
        device_layout.addWidget(device_label)
        device_layout.addWidget(self.device_combo)
        layout.addLayout(device_layout)

        # 控制按钮
        button_layout = QHBoxLayout()
        self.start_button = QPushButton("开始识别")
        self.stop_button = QPushButton("停止识别")
        self.stop_button.setEnabled(False)
        button_layout.addWidget(self.start_button)
        button_layout.addWidget(self.stop_button)
        layout.addLayout(button_layout)

        # 识别结果
        result_label = QLabel("识别结果:")
        layout.addWidget(result_label)
        self.result_text = QTextEdit()
        self.result_text.setReadOnly(True)
        layout.addWidget(self.result_text)

        central_widget.setLayout(layout)

        # 连接信号
        self.start_button.clicked.connect(self.start_recognition)
        self.stop_button.clicked.connect(self.stop_recognition)
        self.source_combo.currentIndexChanged.connect(self.load_devices)

    def load_devices(self):
        self.device_combo.clear()
        source = self.source_combo.currentText()
        if source == "麦克风":
            devices = self.audio_capture.list_microphone_devices()
        else:
            devices = self.audio_capture.list_system_audio_devices()

        for device in devices:
            self.device_combo.addItem(device['name'], device['index'])

    def start_recognition(self):
        source = self.source_combo.currentText()
        device_index = self.device_combo.currentData()

        if source == "麦克风":
            self.audio_capture.start_microphone_capture(device_index)
        else:
            self.audio_capture.start_system_audio_capture(device_index)

        self.audio_worker = AudioWorker(self.audio_capture, self.recognizer)
        self.audio_worker.text_received.connect(self.on_text_received)
        self.audio_worker.start()

        self.start_button.setEnabled(False)
        self.stop_button.setEnabled(True)

    def stop_recognition(self):
        if self.audio_worker:
            self.audio_worker.stop()
            self.audio_worker.wait()
            self.audio_worker = None

        self.audio_capture.stop_capture()

        self.start_button.setEnabled(True)
        self.stop_button.setEnabled(False)

    def on_text_received(self, text):
        self.result_text.append(text)

    def closeEvent(self, event):
        self.stop_recognition()
        event.accept()
```

#### 5.1.3 依赖文件（requirements.txt）

```
faster-whisper>=1.0.0
sounddevice>=0.4.6
PyQt5>=5.15.0
numpy>=1.24.0
scipy>=1.10.0
```

### 5.2 备选方案：Python + Vosk

#### 5.2.1 核心差异

**识别模块（speech_recognizer.py）**

```python
from vosk import Model, KaldiRecognizer
import json
import numpy as np
from typing import Iterator, Optional
import threading
import queue


class VoskSpeechRecognizer:
    def __init__(self, model_path: str):
        self.model = Model(model_path)
        self.recognizer = None
        self.is_running = False
        self.result_queue = queue.Queue()
        self.recognize_thread = None

    def initialize(self, sample_rate: int):
        """初始化识别器"""
        self.recognizer = KaldiRecognizer(self.model, sample_rate)

    def recognize_stream(self, audio_stream: Iterator[np.ndarray]):
        """流式识别"""
        for audio_chunk in audio_stream:
            if not self.is_running:
                break

            # 转换为bytes
            audio_bytes = (audio_chunk * 32767).astype(np.int16).tobytes()

            if self.recognizer.AcceptWaveform(audio_bytes):
                result = json.loads(self.recognizer.Result())
                if result.get('text'):
                    self.result_queue.put({
                        'text': result['text'],
                        'final': True
                    })
            else:
                partial = json.loads(self.recognizer.PartialResult())
                if partial.get('partial'):
                    self.result_queue.put({
                        'text': partial['partial'],
                        'final': False
                    })

    def start_recognize(self, audio_stream: Iterator[np.ndarray]):
        """启动识别线程"""
        self.is_running = True
        self.recognize_thread = threading.Thread(
            target=self.recognize_stream,
            args=(audio_stream,)
        )
        self.recognize_thread.start()

    def stop_recognize(self):
        """停止识别"""
        self.is_running = False
        if self.recognize_thread:
            self.recognize_thread.join()
            self.recognize_thread = None

    def get_result(self, timeout: float = 1.0) -> Optional[dict]:
        """获取识别结果"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None
```

#### 5.2.2 依赖文件（requirements.txt）

```
vosk>=0.3.45
sounddevice>=0.4.6
PyQt5>=5.15.0
numpy>=1.24.0
scipy>=1.10.0
```

---

## 6. 开发指南

### 6.1 环境搭建

#### 6.1.1 安装Python

```bash
# 下载并安装Python 3.8+
# 访问 https://www.python.org/downloads/
```

#### 6.1.2 创建虚拟环境

```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境（Windows）
venv\Scripts\activate

# 激活虚拟环境（Linux/Mac）
source venv/bin/activate
```

#### 6.1.3 安装依赖

```bash
# 安装依赖
pip install -r requirements.txt

# 如果使用Vosk，需要下载模型
# 下载地址：https://alphacephei.com/vosk/models
# 中文模型：vosk-model-small-cn-0.22
```

### 6.2 开发步骤

#### 6.2.1 音频采集测试

```python
# test_audio_capture.py
from audio_capture import AudioCapture

capture = AudioCapture()

# 列出麦克风设备
mic_devices = capture.list_microphone_devices()
print("麦克风设备:")
for device in mic_devices:
    print(f"  {device['index']}: {device['name']}")

# 列出系统音频设备
sys_devices = capture.list_system_audio_devices()
print("\n系统音频设备:")
for device in sys_devices:
    print(f"  {device['index']}: {device['name']}")

# 测试麦克风采集
capture.start_microphone_capture()
print("\n开始采集麦克风音频...")
for i in range(10):
    audio_data = capture.get_audio_data()
    if audio_data is not None:
        print(f"采集到音频数据: {audio_data.shape}")

capture.stop_capture()
```

#### 6.2.2 语音识别测试

```python
# test_speech_recognition.py
from speech_recognizer import SpeechRecognizer
import numpy as np

# 创建识别器
recognizer = SpeechRecognizer(model_size="base")

# 测试识别
test_audio = np.random.randn(16000 * 3).astype(np.float32)  # 3秒音频
segments, info = recognizer.model.transcribe(test_audio, language="zh")

print("识别结果:")
for segment in segments:
    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
```

#### 6.2.3 完整流程测试

```python
# test_full_pipeline.py
from audio_capture import AudioCapture
from audio_processor import AudioProcessor
from speech_recognizer import SpeechRecognizer

# 初始化
capture = AudioCapture()
processor = AudioProcessor()
recognizer = SpeechRecognizer()

# 开始采集
capture.start_microphone_capture()

# 识别循环
print("开始识别...")
try:
    while True:
        audio_data = capture.get_audio_data()
        if audio_data is not None:
            # 处理音频
            processed_audio = processor.process_audio(
                audio_data,
                original_rate=capture.sample_rate
            )

            # 识别
            segments, _ = recognizer.model.transcribe(
                processed_audio,
                language="zh",
                vad_filter=True
            )

            for segment in segments:
                print(f"识别结果: {segment.text}")

except KeyboardInterrupt:
    print("\n停止识别")
    capture.stop_capture()
```

### 6.3 性能优化

#### 6.3.1 音频采集优化

```python
# 优化采样率和缓冲区
class OptimizedAudioCapture(AudioCapture):
    def __init__(self):
        # 使用更低的采样率（16000Hz足够语音识别）
        super().__init__(sample_rate=16000, channels=1)
        # 增大缓冲区大小
        self.chunk_size = 4096
```

#### 6.3.2 识别优化

```python
# 使用更小的模型
recognizer = SpeechRecognizer(model_size="tiny")

# 使用GPU加速
recognizer = SpeechRecognizer(
    model_size="base",
    device="cuda",  # 需要安装CUDA
    compute_type="float16"
)
```

#### 6.3.3 内存优化

```python
# 使用生成器处理音频流
def audio_stream_generator(capture):
    while capture.is_recording:
        audio_data = capture.get_audio_data()
        if audio_data is not None:
            yield audio_data
```

### 6.4 常见问题

#### 6.4.1 音频设备问题

**问题：找不到音频设备**
```python
# 解决方案：检查设备权限
import sounddevice as sd
print(sd.query_devices())
```

**问题：系统音频无法采集**
```python
# 解决方案：使用管理员权限运行
# 或使用pyaudio的WASAPI扩展
```

#### 6.4.2 识别问题

**问题：识别准确率低**
```python
# 解决方案：优化音频预处理
processed_audio = AudioProcessor.process_audio(
    audio_data,
    original_rate=sample_rate,
    apply_noise_reduction=True,
    apply_vad=True
)
```

**问题：识别延迟高**
```python
# 解决方案：使用更小的模型
recognizer = SpeechRecognizer(model_size="tiny")

# 或使用Vosk
recognizer = VoskSpeechRecognizer("model-small-cn")
```

#### 6.4.3 性能问题

**问题：CPU占用高**
```python
# 解决方案：使用GPU加速
recognizer = SpeechRecognizer(
    model_size="base",
    device="cuda"
)

# 或降低采样率
capture = AudioCapture(sample_rate=16000)
```

### 6.5 部署指南

#### 6.5.1 打包为可执行文件

```bash
# 安装PyInstaller
pip install pyinstaller

# 打包
pyinstaller --onefile --windowed main.py

# 打包后文件在dist目录
```

#### 6.5.2 创建安装程序

```bash
# 使用NSIS创建安装程序
# 或使用Inno Setup
```

---

## 附录

### A. 参考资源

#### 官方文档
- [Faster-Whisper文档](https://github.com/guillaumekln/faster-whisper)
- [Vosk文档](https://alphacephei.com/vosk/)
- [SoundDevice文档](https://python-sounddevice.readthedocs.io/)
- [PyAudio文档](https://people.csail.mit.edu/hubert/pyaudio/)

#### Windows音频API
- [WASAPI文档](https://docs.microsoft.com/en-us/windows/win32/coreaudio/wasapi)
- [Windows音频会话API](https://docs.microsoft.com/en-us/windows/win32/coreaudio/windows-audio-session-api)

#### 语音识别论文
- [Attention Is All You Need (Transformer)](https://arxiv.org/abs/1706.03762)
- [Conformer: Convolution-augmented Transformer](https://arxiv.org/abs/2005.08100)
- [Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)](https://arxiv.org/abs/2212.04356)

### B. 术语表

| 术语 | 英文 | 说明 |
|------|------|------|
| 采样率 | Sample Rate | 每秒采样次数，单位Hz |
| 位深 | Bit Depth | 每个采样点的精度 |
| 声道 | Channel | 音频通道数 |
| MFCC | Mel-Frequency Cepstral Coefficients | 梅尔频率倒谱系数 |
| VAD | Voice Activity Detection | 语音活动检测 |
| WASAPI | Windows Audio Session API | Windows音频会话API |
| CTC | Connectionist Temporal Classification | 连接时序分类 |
| Beam Search | 束搜索 | 解码算法 |

### C. 版本历史

| 版本 | 日期 | 说明 |
|------|------|------|
| 1.0 | 2024-01-01 | 初始版本 |

---

**文档结束**
